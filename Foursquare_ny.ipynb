{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\94777\\\\Desktop\\\\covid\\\\foursquare'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('../input/yoochoose-clicks.p','rb') as f: #.p\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv('input/raw/adjacency_matrix.dat', header=None)\n",
    "df.columns=['session_id','timestamp','item_id','price','quantity']\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "buy_df = pd.read_csv('input/yoochoose-buys.dat', header=None)\n",
    "buy_df.columns=['session_id','timestamp','item_id','price','quantity']\n",
    "buy_df.head(20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "buy_df.nunique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df['valid_session'] = df.session_id.map(df.groupby('session_id')['item_id'].size() > 2)\n",
    "df = df.loc[df.valid_session].drop('valid_session',axis=1)\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# #randomly sample a couple of them\n",
    "sampled_session_id = np.random.choice(df.session_id.unique(), 3000, replace=False)\n",
    "df = df.loc[df.session_id.isin(sampled_session_id)]\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# average length of session \n",
    "df.groupby('session_id')['item_id'].size().mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "item_encoder = LabelEncoder()\n",
    "df['item_id'] = item_encoder.fit_transform(df.item_id)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df['label'] = df.session_id.isin(buy_df.session_id)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.drop_duplicates('session_id')['label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class YooChooseBinaryDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(YooChooseBinaryDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['adjacency_matrix.npy']\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['processed.dat']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        \n",
    "        data_list = []\n",
    "        \n",
    "        for raw_path in self.raw_paths:\n",
    "            # Read data from `raw_path`.\n",
    "            temporal_network = np.load(raw_path).squeeze().T[:, :100,:100]\n",
    "            print(temporal_network.shape)\n",
    "            for network in temporal_network:\n",
    "                df = pd.DataFrame(network)\n",
    "                df = df.stack().reset_index()\n",
    "\n",
    "                edge_list = np.array(df[['level_0', 'level_1' ]]).T            \n",
    "                edge_index = torch.tensor(edge_list, dtype=torch.long)\n",
    "                #x = torch.zeros(network.shape[0])\n",
    "                y = torch.zeros(network.shape[0]) #total number of infected people\n",
    "                y[0]=1\n",
    "                data = Data( edge_index=edge_index, y=y) #x=x,\n",
    "                data_list.append(data)\n",
    "        print(np.array(data_list).shape)\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = YooChooseBinaryDataset(root='../foursquare/input')\n",
    "labels = [0, 1]\n",
    "dataset[1].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30, 40)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.shuffle()\n",
    "train_dataset = dataset[:30]\n",
    "val_dataset = dataset[30:60]\n",
    "test_dataset = dataset[60:]\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "batch_size= 1 #1024 ammar\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_max = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n",
      "Batch(batch=[100], edge_index=[2, 10000], y=[100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\94777\\.conda\\envs\\env_pytorch\\lib\\site-packages\\torch_geometric\\data\\data.py:184: UserWarning: The number of nodes in your data object can only be inferred by its edge indices, and hence may result in unexpected batch-wise behavior, e.g., in case there exists isolated nodes. Please consider explicitly setting the number of nodes for this data object by assigning it to data.num_nodes.\n",
      "  warnings.warn(__num_nodes_warn_msg__.format('edge'))\n"
     ]
    }
   ],
   "source": [
    "for t_images in train_loader:\n",
    "    print(t_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C:\\Users\\94777\\.conda\\envs\\env_pytorch\\lib\\site-packages\\torch_geometric\\data\\data.py:184: UserWarning: The number of nodes in your data object can only be inferred by its edge indices, and hence may result in unexpected batch-wise behavior, e.g., in case there exists isolated nodes. Please consider explicitly setting the number of nodes for this data object by assigning it to data.num_nodes.\n",
    "  warnings.warn(__num_nodes_warn_msg__.format('edge'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops\n",
    "class SAGEConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SAGEConv, self).__init__(aggr='max') #  \"Max\" aggregation.\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.update_lin = torch.nn.Linear(in_channels + out_channels, in_channels, bias=False)\n",
    "        self.update_act = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "        \n",
    "        \n",
    "        edge_index, _ = remove_self_loops(edge_index)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        \n",
    "        \n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        # x_j has shape [E, in_channels]\n",
    "\n",
    "        x_j = self.lin(x_j)\n",
    "        x_j = self.act(x_j)\n",
    "        \n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "\n",
    "\n",
    "        new_embedding = torch.cat([aggr_out, x], dim=1)\n",
    "        \n",
    "        new_embedding = self.update_lin(new_embedding)\n",
    "        new_embedding = self.update_act(new_embedding)\n",
    "        \n",
    "        return new_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GraphConv, TopKPooling, GatedGraphConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "import torch.nn.functional as F\n",
    "from pygcn.utils import load_data, accuracy\n",
    "from pygcn.models import GCN\n",
    "\n",
    "embed_dim = 128\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_embeddings = 1083, batch_size=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = SAGEConv(embed_dim, 1)\n",
    "        self.pool1 = TopKPooling(1, ratio=1)\n",
    "        \n",
    "        self.item_embedding = torch.nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embed_dim)\n",
    "        \n",
    "        self.conv2 = SAGEConv(128, 1)\n",
    "        self.pool2 = TopKPooling(1, ratio=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128,1)\n",
    "        self.fc1_act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        y, edge_index, batch = data.y, data.edge_index, data.batch\n",
    "        y = self.item_embedding(y.type(torch.LongTensor))\n",
    "        y = y.squeeze(1)        \n",
    "\n",
    "        y = F.relu(self.conv1(y, edge_index)) \n",
    "        y, edge_index, _, batch,_, _= self.pool1(y, edge_index, None, batch)\n",
    "        \n",
    "        y = F.relu(self.conv2(y, edge_index))\n",
    "        y, edge_index, _, batch,_, _= self.pool2(y, edge_index, None, batch)\n",
    "        \n",
    "        y =  self.fc1_act(self.fc1(y)).squeeze()\n",
    "        #x = F.relu(self.conv2(x, edge_index))\n",
    "        #x, edge_index, _, batch,_, _= self.pool2(x, edge_index, None, batch)\n",
    "        ## insert\n",
    "        return y , edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cpu') #ammar\n",
    "model = Net(num_embeddings=128).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "crit = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  torch.Size([100])\n",
      "input_edge_index torch.Size([2, 10000])\n",
      "output_edge_index : torch.Size([2, 10000])\n",
      "output :  torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        print('input : ', data.y.shape)\n",
    "        print('input_edge_index', data.edge_index.shape)\n",
    "        output, edge_index = model(data)\n",
    "        label = data.y.to(device)\n",
    "        #loss = crit(output , label.type(torch.FloatTensor))\n",
    "        print('output_edge_index :' , edge_index.shape)\n",
    "        print('output : ', output.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input edge_index : 10000 [[ 0  0  0 ... 99 99 99]\n",
      " [ 0  1  2 ... 97 98 99]]\n",
      "output edge_index : 10000  --- [[36 36 36 ... 99 99 99]\n",
      " [36 11  2 ... 97 98 99]]\n",
      "input nodes : 100 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "output nodes : 100 ---  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('input edge_index :', len(data.edge_index.detach().numpy().T), data.edge_index.detach().numpy())\n",
    "print('output edge_index :', len(edge_index.detach().numpy().squeeze().T), ' ---'  ,edge_index.detach().numpy().squeeze())\n",
    "\n",
    "print('input nodes :', len(data.y.detach().numpy()), data.y.detach().numpy())\n",
    "print('output nodes :', len(output.detach().numpy()), '--- '  ,output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "graph = nx.from_edgelist(edge_index.detach().numpy().T.squeeze())\n",
    "nx.number_connected_components(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    edge_index= 0\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "   edge_index= 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        label = data.y.to(device)\n",
    "        loss = crit(output , label.type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            pred = model(data).detach().cpu().numpy()\n",
    "\n",
    "            label = data.y.detach().cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "            labels.append(label)\n",
    "\n",
    "    if(len(predictions) > 0):\n",
    "        predictions = np.hstack(predictions)\n",
    "        labels = np.hstack(labels)\n",
    "        print('labels: ',labels)\n",
    "        print('predictions: ',predictions)\n",
    "        try:\n",
    "            return roc_auc_score(labels, predictions)\n",
    "        except ValueError: \n",
    "            print('roc_auc_score error')\n",
    "            return 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(epochs):\n",
    "    train_losses = []\n",
    "    train_accs =  []\n",
    "    val_accs = []\n",
    "    test_accs =  []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss = train()\n",
    "        train_acc = evaluate(train_loader)\n",
    "        #val_acc = evaluate(val_loader)    \n",
    "        #test_acc = evaluate(test_loader)\n",
    "\n",
    "        train_losses.append(loss)\n",
    "        #val_accs.append(val_acc)\n",
    "        #test_accs.append(test_acc)\n",
    "        \n",
    "        print('Epoch: {:03d}, Loss: {:.5f}, Train Auc: {:.5f}, Val Auc: {:.5f}, Test Auc: {:.5f}'.\n",
    "              format(epoch, loss, train_acc, 0, 0))# val_acc, test_acc))\n",
    "    \n",
    "    return train_losses, None, None #val_accs , test_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for data in train_loader:\n",
    "    data = data.to(device)\n",
    "    output = model(data)\n",
    "    print(output)\n",
    "    label = data.y.to(device)\n",
    "    print(label)\n",
    "    #loss = crit(output, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#output.type(torch.LongTensor)\n",
    "crit(output , label.type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses,  val_accs , test_accs = run(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def normal_values(train_losses):\n",
    "    norm1 = np.array(train_losses) / np.linalg.norm(train_losses)\n",
    "    #norm2 = normalize(np.array(train_losses)[:,np.newaxis], axis=0).ravel()\n",
    "    return norm1\n",
    "\n",
    "train_line, = plt.plot(normal_values(train_losses))\n",
    "train_line.set_label('training')\n",
    "#plt.plot(normal_values(val_accs))\n",
    "\n",
    "#test_accs_line, = plt.plot(normal_values(test_accs))\n",
    "#test_accs_line.set_label('test')\n",
    "\n",
    "#validation_line, = plt.plot(normal_values(val_accs))\n",
    "#validation_line.set_label('validation')\n",
    "#plt.plot(normal_values(val_accs))\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
