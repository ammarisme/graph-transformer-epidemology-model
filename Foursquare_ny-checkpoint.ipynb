{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class YooChooseBinaryDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(YooChooseBinaryDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['adjacency_matrix.npy']\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['processed.dat']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        \n",
    "        data_list = []\n",
    "        \n",
    "        for raw_path in self.raw_paths:\n",
    "            # Read data from `raw_path`.\n",
    "            temporal_network = np.load(raw_path).squeeze().T[:, :100,:100]\n",
    "            for network in temporal_network:\n",
    "                df = pd.DataFrame(network)\n",
    "                df = df.stack().reset_index()\n",
    "\n",
    "                edge_list = np.array(df[['level_0', 'level_1' ]]).T            \n",
    "                edge_index = torch.tensor(edge_list, dtype=torch.long)\n",
    "\n",
    "                x = torch.zeros(100)\n",
    "                data = Data(x=x, edge_index=edge_index, y=x)\n",
    "                data_list.append(data)\n",
    "        \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "        \n",
    "        \"\"\"\n",
    "        # process by session_id\n",
    "        grouped = df.groupby('session_id')\n",
    "        for session_id, group in tqdm(grouped):\n",
    "            sess_item_id = LabelEncoder().fit_transform(group.item_id)\n",
    "            group = group.reset_index(drop=True)\n",
    "            group['sess_item_id'] = sess_item_id\n",
    "            node_features = group.loc[group.session_id==session_id,['sess_item_id','item_id']].sort_values('sess_item_id').item_id.drop_duplicates().values\n",
    "\n",
    "            node_features = torch.LongTensor(node_features).unsqueeze(1)\n",
    "            target_nodes = group.sess_item_id.values[1:]\n",
    "            source_nodes = group.sess_item_id.values[:-1]\n",
    "\n",
    "            edge_index = torch.tensor([source_nodes,\n",
    "                                   target_nodes], dtype=torch.long)\n",
    "            x = node_features\n",
    "\n",
    "            y = torch.FloatTensor([group.label.values[0]])\n",
    "\n",
    "            data = Data(x=x, edge_index=edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "            \"\"\" \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = YooChooseBinaryDataset(root='input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('input/raw/data-4SfjZ.csv')\n",
    "y = np.array(y['Cases'])\n",
    "y = y[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle()\n",
    "train_dataset = dataset[:30]\n",
    "val_dataset = dataset[30:60]\n",
    "test_dataset = dataset[60:]\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "batch_size= 1 #1024 ammar\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_items = df.item_id.max() +1 \n",
    "num_items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_max = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_images in train_loader:\n",
    "    print(t_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops\n",
    "class SAGEConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SAGEConv, self).__init__(aggr='max') #  \"Max\" aggregation.\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.update_lin = torch.nn.Linear(in_channels + out_channels, in_channels, bias=False)\n",
    "        self.update_act = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]     \n",
    "        edge_index, _ = remove_self_loops(edge_index)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        \n",
    "        \n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        # x_j has shape [E, in_channels]\n",
    "\n",
    "        x_j = self.lin(x_j)\n",
    "        x_j = self.act(x_j)\n",
    "        \n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "\n",
    "\n",
    "        new_embedding = torch.cat([aggr_out, x], dim=1)\n",
    "        \n",
    "        new_embedding = self.update_lin(new_embedding)\n",
    "        new_embedding = self.update_act(new_embedding)\n",
    "        \n",
    "        return new_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "class Chimera(nn.Module):\n",
    "    def __init__(self, K = 10, N = 100):\n",
    "        super(Chimera, self).__init__()\n",
    "        self.U = torch.randn(N, K, requires_grad= True)\n",
    "        self.V = torch.randn(K, N, requires_grad= True)\n",
    "        self.params = nn.ParameterList([nn.Parameter(self.U),nn.Parameter(self.V)])\n",
    "    \n",
    "    def forward(self, At):\n",
    "        output =  torch.mm(self.U , self.V)\n",
    "        return output\n",
    "    \n",
    "def mapping(x):\n",
    "    cache = {}\n",
    "    if x not in cache:\n",
    "        cache[x] = len(cache)\n",
    "    return cache[x]\n",
    "\n",
    "def tree_estimate(U):\n",
    "    kmeans = KMeans(n_clusters=2, init='k-means++', n_init=100, max_iter=3000, tol=0.0001 )\n",
    "    labels = kmeans.fit_predict(U.detach().numpy().reshape((-1, U.detach().numpy().shape[-1])))\n",
    "    counts = np.bincount(np.array([mapping(l) for l in labels]))\n",
    "\n",
    "    if len(counts) > 0:\n",
    "        first_community = np.argmax(counts)\n",
    "        counts = np.bincount(np.delete(counts, first_community))\n",
    "        first_community = np.sum(np.where(labels == first_community, 1, 0))\n",
    "    if len(counts) > 0:\n",
    "        second_community = np.argmax(counts)\n",
    "        second_community = np.sum(np.where(labels == second_community, 1, 0))\n",
    "    else : \n",
    "        second_community = 0\n",
    "\n",
    "    ratio = 1 - ((first_community - second_community) / first_community)\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "from torch_geometric.nn import GraphConv, TopKPooling, GatedGraphConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, num_embeddings = 1083, batch_size=10):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = 100\n",
    "        self.conv1 = SAGEConv(embed_dim, 100)\n",
    "        self.pool1 = TopKPooling(1, ratio=1)\n",
    "        self.lin1 = nn.Linear(128*100, 100)\n",
    "        self.lin1_act = nn.ReLU()\n",
    "        self.rec1 = nn.GRU(input_size = 100, hidden_size = 1,  batch_first =True)\n",
    "        self.rec1_act = nn.ReLU()\n",
    "        self.hidden_h0 = self.initial_h0()\n",
    "        self.item_embedding = torch.nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embed_dim)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.y, data.edge_index, data.batch\n",
    "        x = self.item_embedding(x.type(torch.LongTensor))\n",
    "        x = x.squeeze(1)        \n",
    "        \n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch,_, _= self.pool1(x, edge_index, None, batch) \n",
    "        x = torch.flatten(x)\n",
    "        \n",
    "        x = self.lin1(x)\n",
    "        x= self.lin1_act(x)\n",
    "        \n",
    "        return x , edge_index\n",
    "    \n",
    "    def initial_h0(self):\n",
    "        h0 = torch.randn(2, 3, 20)\n",
    "        return h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "from torch_geometric.nn import GraphConv, TopKPooling, GatedGraphConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, num_embeddings = 1083, batch_size=10):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lin0 = nn.Linear(100, 1)\n",
    "        self.lin0_act = nn.ReLU()\n",
    "        \n",
    "        self.lin0_1 = nn.Linear(100,1)\n",
    "        self.lin0_1_act = nn.ReLU()\n",
    "        \n",
    "        self.hidden_size = 100\n",
    "        self.conv1 = SAGEConv(embed_dim, 100)\n",
    "        self.pool1 = TopKPooling(1, ratio=1)\n",
    "        self.lin1 = nn.Linear(128*100, 100)\n",
    "        self.lin1_act = nn.ReLU()\n",
    "        self.rec1 = nn.GRU(input_size = 100, hidden_size = 100,  batch_first =True)\n",
    "        self.rec1_act = nn.ReLU()\n",
    "        self.hidden_h0 = self.initial_h0()\n",
    "        self.item_embedding = torch.nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embed_dim)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.y, data.edge_index, data.batch\n",
    "        x = self.lin0(x)\n",
    "        x = x.squeeze()\n",
    "        x = self.lin0_act(x)\n",
    "        \n",
    "        \n",
    "        edge_index =edge_index.type(torch.FloatTensor).view(2,10000, 100)\n",
    "        edge_index = self.lin0_1(edge_index)\n",
    "        edge_index = self.lin0_1_act(edge_index)\n",
    "        edge_index = edge_index.squeeze()\n",
    "        \n",
    "        \n",
    "        x = self.item_embedding(x.type(torch.LongTensor))\n",
    "        x = x.squeeze(1)        \n",
    "        \n",
    "        edge_index = edge_index.type(torch.LongTensor)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "\n",
    "        #x, edge_index, _, batch,_, _= self.pool1(x, edge_index, None, batch) \n",
    "        \n",
    "        x = torch.flatten(x)\n",
    "        \n",
    "        x = self.lin1(x)\n",
    "        x= self.lin1_act(x)\n",
    "        \n",
    "        return x , edge_index\n",
    "    \n",
    "    def initial_h0(self):\n",
    "        h0 = torch.randn(2, 3, 20)\n",
    "        return h0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, edge_index = model(data)\n",
    "        label = data.y.to(device)\n",
    "        #loss = crit(output , label.type(torch.FloatTensor))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor,target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=100, coefficient = 100, target_length = 30):\n",
    "    encoder_edge_index = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = len(train_loader) #input_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    encoder_edge_indexes = torch.zeros(max_length, 2,10000,  device=device)\n",
    "    loss = 0\n",
    "    \n",
    "\n",
    "    #for ei in range(input_length):\n",
    "    i = 0\n",
    "    for data in train_loader:\n",
    "        encoder_output, encoder_edge_index = encoder(data)\n",
    "        encoder_outputs[i] = encoder_output\n",
    "        encoder_edge_indexes[i] = encoder_edge_index\n",
    "        i+=1\n",
    "        \n",
    "    decoder_input = torch.tensor([[10000]], device=device)\n",
    "\n",
    "    decoder_edge_index = encoder_edge_indexes\n",
    "    decoder_outputs = []\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    #delta = np.random.uniform(-10,10, size=(100,))\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            batch = torch.tensor(100, device=device)\n",
    "            data = Data(y = encoder_outputs, edge_index = encoder_edge_indexes, batch = batch)\n",
    "            decoder_output, decoder_edge_index = decoder(data)\n",
    "            loss += criterion(torch.sum(decoder_output)*coefficient,torch.sum(target_tensor[di]))\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            batch = torch.randn(100, device=device)\n",
    "            data = Data(y = encoder_outputs, edge_index = encoder_edge_indexes, batch = batch)\n",
    "            decoder_output, decoder_edge_index = decoder(data)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            loss += criterion(torch.sum(decoder_output)*coefficient,torch.sum(target_tensor[di]))\n",
    "            decoder_outputs.append(decoder_output)\n",
    "        \n",
    "    tree_estimator = Chimera(K = 10, N = 100)\n",
    "    tree_estimator_criterion = nn.MSELoss()\n",
    "    tree_estimator_optimizer = torch.optim.Adam(tree_estimator.parameters(), lr = 0.01)\n",
    "    #do the tree estimation\n",
    "    if(len(decoder_outputs) > 0):\n",
    "        decoder_outputs = torch.stack(decoder_outputs)\n",
    "        for i in range(2):\n",
    "            for a in decoder_outputs:\n",
    "                tree_estimator_optimizer.zero_grad()\n",
    "                UV = tree_estimator(a)\n",
    "                Us = tree_estimator.U\n",
    "                loss += tree_estimator_criterion(torch.tensor(tree_estimate(Us)) , torch.tensor([1]))\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    tree_estimator_optimizer.step()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu') #ammar\n",
    "encoder = Encoder(num_embeddings=100).to(device)\n",
    "decoder = Decoder(num_embeddings=100).to(device)\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters() ,lr = 0.01)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters() ,lr = 0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "MAX_LENGTH= 100\n",
    "target_tensor = torch.from_numpy(y).type(torch.FloatTensor)\n",
    "train(dataset,target_tensor, encoder,decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=100,target_length = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def trainIters(input_tensor,target_tensor,encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        plot_losses.append(loss)\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "    return plot_losses , plot_loss_total\n",
    "    \n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses, p = trainIters(dataset,target_tensor,encoder, decoder, 50, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def predict(input_tensor, encoder, decoder, max_length=100, coefficient = 100, target_length = 30):\n",
    "    encoder_edge_index = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    input_length = len(train_loader) #input_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    encoder_edge_indexes = torch.zeros(max_length, 2,10000,  device=device)\n",
    "    loss = 0\n",
    "    \n",
    "\n",
    "    #for ei in range(input_length):\n",
    "    i = 0\n",
    "    for data in train_loader:\n",
    "        encoder_output, encoder_edge_index = encoder(data)\n",
    "        encoder_outputs[i] = encoder_output\n",
    "        encoder_edge_indexes[i] = encoder_edge_index\n",
    "        i+=1\n",
    "        \n",
    "    #decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_input = torch.tensor([[10000]], device=device)\n",
    "\n",
    "    decoder_edge_index = encoder_edge_indexes\n",
    "    decoder_outputs = []\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    delta = np.random.uniform(-10,10, size=(100,))\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            batch = torch.tensor(100, device=device)\n",
    "            data = Data(y = encoder_outputs, edge_index = encoder_edge_indexes, batch = batch)\n",
    "            decoder_output, decoder_edge_index = decoder(data)\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            batch = torch.randn(100, device=device)\n",
    "            data = Data(y = encoder_outputs, edge_index = encoder_edge_indexes, batch = batch)\n",
    "            decoder_output, decoder_edge_index = decoder(data)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            decoder_outputs.append(torch.sum(decoder_output)*coefficient)\n",
    "            #if decoder_input.item() == EOS_token:\n",
    "            #    break\n",
    "    return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu') #ammar\n",
    "predict(val_dataset, encoder,decoder, max_length=100,target_length = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def normal_values(train_losses):\n",
    "    norm1 = np.array(train_losses) / np.linalg.norm(train_losses)\n",
    "    #norm2 = normalize(np.array(train_losses)[:,np.newaxis], axis=0).ravel()\n",
    "    return norm1\n",
    "\n",
    "train_line, = plt.plot(normal_values(train_losses))\n",
    "train_line.set_label('training')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
